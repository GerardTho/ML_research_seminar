[
  {
    "objectID": "visualisation/Resultats_graphiques_GNN.html",
    "href": "visualisation/Resultats_graphiques_GNN.html",
    "title": "I. Visualisation plot",
    "section": "",
    "text": "from plot import visualisation_plot, prepare_data, to_table, save_plot\n%matplotlib widget\nlist_dict = prepare_data.get_list_dict()\nvplot = visualisation_plot.VisualisationPlot(list_dict)"
  },
  {
    "objectID": "visualisation/Resultats_graphiques_GNN.html#scatter-plot-in-3d",
    "href": "visualisation/Resultats_graphiques_GNN.html#scatter-plot-in-3d",
    "title": "I. Visualisation plot",
    "section": "1) Scatter plot in 3D",
    "text": "1) Scatter plot in 3D\n\nvplot.plot_from_dict(alpha=.7);"
  },
  {
    "objectID": "visualisation/Resultats_graphiques_GNN.html#pairplot",
    "href": "visualisation/Resultats_graphiques_GNN.html#pairplot",
    "title": "I. Visualisation plot",
    "section": "2) Pairplot",
    "text": "2) Pairplot\n\nlist_dict[0].keys()\n\ndict_keys(['alpha', 'attention_heads', 'batch_size', 'convolution_layer', 'dataset', 'deterministic_algorithms', 'global_pooling_layer', 'local_pooling_layer', 'lr', 'max_epochs', 'patience', 'split 1', 'split 2', 'split 3', 'split 4', 'split 5', 'split 6', 'split 7', 'split 8', 'split 9', 'split 10', 'nb_parameters', 'mean_accuracy', 'std_accuracy', 'Training time', 'homophily', 'pooling_and_archi', 'avg_nodes', 'avg_edges'])\n\n\n\nvplot.pairplot_from_dict(\n                   [\n                       ('avg_nodes','mean_accuracy'),\n                       ('avg_edges','mean_accuracy'),\n                    ],\n                   dataset=None,\n                   dim_grid_subplots=(2,1),\n                   figsize=(8,9),\n                   plot=False,\n                   kwargs1={'alpha' : .7, 's' : 30.},\n                   kwargs2={'bbox_to_anchor' : (1.01,1)},\n                   kwargs3={'alpha' : .7, 'linestyle' : ':', 'linewidth' : .7},\n                   padding_subplots=.07\n                   )\n\n\nvplot.pairplot_from_dict(\n                   [\n                       ('Training time','mean_accuracy'),\n                       ('nb_parameters','mean_accuracy'),\n                    ],\n                   dataset=\"MUTAG\",\n                   dim_grid_subplots=(2,1),\n                   figsize=(8,9),\n                   plot=False,\n                   kwargs1={'alpha' : .7, 's' : 30.},\n                   kwargs2={'bbox_to_anchor' : (1.01,1)},\n                   kwargs3={'alpha' : .7, 'linestyle' : ':', 'linewidth' : .7},\n                   padding_subplots=.07\n                   )"
  },
  {
    "objectID": "visualisation/Resultats_graphiques_GNN.html#barplot",
    "href": "visualisation/Resultats_graphiques_GNN.html#barplot",
    "title": "I. Visualisation plot",
    "section": "3) Barplot",
    "text": "3) Barplot\n\nvplot.plot_bar_dataset(groupby=\"local_pooling_layer\",\n                       stack=None,\n                       bar_width=0.12,\n                       x_figsize=12,\n                       padding_subplots=.005);\n\n\n\n\n\nvplot.plot_bar_dataset(groupby=\"convolution_layer\",\n                       stack=None,\n                       bar_width=0.4,\n                       x_figsize=12,\n                       padding_subplots=.005);\n\n\n\n\n\nvplot.plot_bar_dataset(groupby=\"local_pooling_layer\",\n                       stack=\"convolution_layer\",\n                       bar_width=0.3,\n                       offset = .02,\n                       x_figsize=12,\n                       padding_subplots=.005);\n\n\n\n\n\nvplot.plot_bar_dataset(groupby=\"convolution_layer\",\n                       stack=\"local_pooling_layer\",\n                       bar_width=0.07,\n                       offset=0.01,\n                       x_figsize=12,\n                       kwargs2={'bbox_to_anchor' : (1.01,1)},\n                       padding_subplots=.005);"
  },
  {
    "objectID": "notebooks/gnn_mewispool.html",
    "href": "notebooks/gnn_mewispool.html",
    "title": "Define Models",
    "section": "",
    "text": "%%capture capt\n!pip install matplotlib\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GINConv, global_mean_pool\nfrom torch_geometric.utils import get_laplacian\n\nimport sys\nsys.path.append('../model')\nfrom mewisPool_layer import MLP, MEWISPool\n\n\ntorch.use_deterministic_algorithms(True)\ntorch.backends.cudnn.deterministic = True\n\n\nclass Net(nn.Module):\n    def __init__(self, input_dim, hidden_dim, num_classes, device):\n        super(Net, self).__init__()\n\n        self.gc1 = GINConv(MLP(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=hidden_dim, enhance=True))\n        self.pool1 = MEWISPool(hidden_dim=hidden_dim, device=device)\n        \n        self.gc2 = GINConv(MLP(input_dim=hidden_dim, hidden_dim=hidden_dim, output_dim=hidden_dim, enhance=True))\n        self.pool2 = MEWISPool(hidden_dim=hidden_dim, device=device)\n        \n        self.gc3 = GINConv(MLP(input_dim=hidden_dim, hidden_dim=hidden_dim, output_dim=hidden_dim, enhance=True))\n        \n        self.fc1 = nn.Linear(in_features=hidden_dim, out_features=hidden_dim)\n        self.fc2 = nn.Linear(in_features=hidden_dim, out_features=num_classes)\n\n    def forward(self, x, edge_index, batch):\n        x = F.relu(self.gc1(x, edge_index))\n\n        x_pooled, edge_index_pooled, batch_pooled, loss1 = self.pool1(x, edge_index, batch)\n\n        x_pooled = F.relu(self.gc2(x_pooled, edge_index_pooled))\n\n        x_pooled, edge_index_pooled, batch_pooled, loss2 = self.pool2(x_pooled, edge_index_pooled,\n                                                                                batch_pooled)\n\n        x_pooled = self.gc3(x_pooled, edge_index_pooled)\n        # readout = global_mean_pool(x_pooled, batch_pooled)\n        readout = torch.cat([x_pooled[batch_pooled == i].mean(0).unsqueeze(0) for i in torch.unique(batch_pooled)],\n                    dim=0)\n\n        out = self.fc1(readout)\n        out = torch.relu(out)\n        out = self.fc2(out)\n\n        return out, loss1 + loss2\n\n\nclass Net_complex(nn.Module):\n    def __init__(self, input_dim, hidden_dim, num_classes, device):\n        super(Net_complex, self).__init__()\n\n        self.gc1 = GINConv(MLP(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=hidden_dim, enhance=True))\n        self.gc2 = GINConv(MLP(input_dim=hidden_dim, hidden_dim=hidden_dim, output_dim=hidden_dim, enhance=True))\n        self.pool1 = MEWISPool(hidden_dim=hidden_dim, device=device)\n        \n        self.gc3 = GINConv(MLP(input_dim=hidden_dim, hidden_dim=hidden_dim, output_dim=hidden_dim, enhance=True))\n        self.gc4 = GINConv(MLP(input_dim=hidden_dim, hidden_dim=hidden_dim, output_dim=hidden_dim, enhance=True))\n        self.pool2 = MEWISPool(hidden_dim=hidden_dim, device=device)\n        \n        self.gc5 = GINConv(MLP(input_dim=hidden_dim, hidden_dim=hidden_dim, output_dim=hidden_dim, enhance=True))\n        \n        self.fc1 = nn.Linear(in_features=hidden_dim, out_features=hidden_dim)\n        self.fc2 = nn.Linear(in_features=hidden_dim, out_features=num_classes)\n\n    def forward(self, x, edge_index, batch):\n        x = self.gc1(x, edge_index)\n        x = torch.relu(x)\n\n        x = self.gc2(x, edge_index)\n        x = torch.relu(x)\n\n        x_pooled1, edge_index_pooled1, batch_pooled1, loss1 = self.pool1(x, edge_index, batch)\n\n        x_pooled1 = self.gc3(x_pooled1, edge_index_pooled1)\n        x_pooled1 = torch.relu(x_pooled1)\n\n        x_pooled1 = self.gc4(x_pooled1, edge_index_pooled1)\n        x_pooled1 = torch.relu(x_pooled1)\n\n        x_pooled2, edge_index_pooled2, batch_pooled2, loss2 = self.pool2(x_pooled1, edge_index_pooled1,\n                                                                                 batch_pooled1)\n\n        x_pooled2 = self.gc5(x_pooled2, edge_index_pooled2)\n        x_pooled2 = torch.relu(x_pooled2)\n\n        readout = torch.cat([x_pooled2[batch_pooled2 == i].mean(0).unsqueeze(0) for i in torch.unique(batch_pooled2)],\n                            dim=0)\n\n        out = self.fc1(readout)\n        out = torch.relu(out)\n        out = self.fc2(out)\n\n        return out, loss1 + loss2\n\n\nApply on MUTAG\n\nLoad data\n\nfrom torch_geometric.datasets import TUDataset\nfrom torch_geometric.loader import DataLoader\n\n\npath = 'data'\ndataset = TUDataset(path, name='MUTAG', use_node_attr=True, use_edge_attr=True).shuffle()\n\ninput_dim = dataset.num_features\nnum_classes = dataset.num_classes\n\nDownloading https://www.chrsmrrs.com/graphkerneldatasets/MUTAG.zip\nProcessing...\nDone!\n\n\n\ndef get_dataloaders(BATCH_SIZE=20, VAL_PCT=0.1, TEST_PCT=0.1):\n    BATCH_SIZE = 20\n    VAL_PCT = 0.1\n    TEST_PCT = 0.1\n    \n    n_train = int(len(dataset) * (1 - VAL_PCT - TEST_PCT))\n    n_val = int(len(dataset) * VAL_PCT)\n    \n    train_dataset = dataset[:n_train]\n    val_dataset = dataset[n_train: n_train+n_val]\n    test_dataset = dataset[n_train+n_val:]\n    \n    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n\n    return train_loader, val_loader, test_loader\n\n\nBATCH_SIZE = 20\nVAL_PCT = 0.15\nTEST_PCT = 0.1\n\ntorch.manual_seed(42)\nnp.random.seed(42)\n\ntrain_loader, val_loader, test_loader = get_dataloaders(BATCH_SIZE=BATCH_SIZE, VAL_PCT=VAL_PCT, TEST_PCT=TEST_PCT)\nprint(f'{len(train_loader.dataset)} training samples, {len(val_loader.dataset)} validation samples, '\n      f'{len(test_loader.dataset)} test samples.')\n\n150 training samples, 18 validation samples, 20 test samples.\n\n\n\ndevice = torch.device('cpu')\nprint(f'Using device: {device}')\n\nUsing device: cpu\n\n\n\n\nLoad model architectures\n\nhidden_dim = 32\nnb_epochs = 200\nlearning_rate = 1e-4\nweight_decay = 1e-5\nscheduler_patience = 10\nscheduler_factor = 1e-1\npatience = 50\n\n\ndef create_model(model_architecture, input_dim=input_dim, hidden_dim=hidden_dim, num_classes=num_classes, device=device, \n        learning_rate=learning_rate, weight_decay=weight_decay, scheduler_patience=scheduler_patience, scheduler_factor=scheduler_factor, verbose_scheduler=True):\n\n    model = model_architecture(input_dim=input_dim, \n                hidden_dim=hidden_dim, num_classes=num_classes, device=device).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n                                                           patience=scheduler_patience,\n                                                           factor=scheduler_factor,\n                                                           verbose=verbose_scheduler)\n    criterion = torch.nn.CrossEntropyLoss()\n    return model, optimizer, scheduler, criterion\n\n\n\nTrain models\n\ndef validate(model, val_loader, criterion, device):\n    val_loss = 0.\n    val_corrects = 0\n    model.eval()\n    with torch.no_grad():\n        for i, data in enumerate(val_loader):\n            data = data.to(device)\n            out, loss_pool = model(data.x, data.edge_index, data.batch)\n            loss_classification = criterion(out, data.y)\n            loss = loss_classification + 0.01 * loss_pool\n            val_loss += loss.item()\n            val_corrects += (F.softmax(out, dim=1).argmax(dim=1) == data.y).sum().item()\n\n    val_loss /= len(val_loader)\n    val_acc = val_corrects / len(val_loader.dataset)\n    return val_loss, val_acc\n\n\ndef train_epoch(model, train_loader, criterion, optimizer, scheduler, device):\n    train_loss = 0.\n    train_corrects = 0\n    model.train()\n    for i, data in enumerate(train_loader):\n        data = data.to(device)\n        optimizer.zero_grad()\n        out, loss_pool = model(data.x, data.edge_index, data.batch)\n        loss_classification = criterion(out, data.y)\n        loss = loss_classification + 0.01 * loss_pool\n\n        loss.backward()\n        train_loss += loss.item()\n        train_corrects += (F.softmax(out, dim=1).argmax(dim=1) == data.y).sum().item()\n        optimizer.step()\n\n    train_loss /= len(train_loader)\n    train_acc = train_corrects / len(train_loader.dataset)\n    scheduler.step(train_loss)\n    return train_loss, train_acc\n\n\ndef train(model, criterion, optimizer, scheduler, train_loader, val_loader, test_loader, nb_epochs, patience, device,\n         print_every=None):\n\n    best_val_loss = float('inf')\n    best_test_acc = None\n    counter = 0\n\n    train_losses,  val_losses, test_losses = [], [], []\n    train_accs, val_accs, test_accs = [], [], []\n\n    if print_every is None:\n        print_every = 1 if nb_epochs &lt;= 20 else int(nb_epochs/20)\n\n    for epoch in range(nb_epochs):\n        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, scheduler, device)\n        val_loss, val_acc = validate(model, val_loader, criterion, device)\n        test_loss, test_acc = validate(model, test_loader, criterion, device)\n\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        test_losses.append(test_loss)\n        train_accs.append(train_acc)\n        val_accs.append(val_acc)\n        test_accs.append(test_acc)\n\n        if epoch % print_every == 0 or epoch == nb_epochs - 1:\n            print(f\"Epoch [{epoch}/{nb_epochs}], Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, Test loss: {test_loss:.3f},\"\n                  f\" Train acc: {train_acc:.2f}, Val acc: {val_acc:.2f}, Test acc: {test_acc:.2f}\")\n\n        # Early-Stopping\n        if val_loss &lt; best_val_loss:\n            best_val_loss = val_loss\n            best_test_acc = test_acc\n            counter = 0\n        else:\n            counter += 1\n\n        # Early stopping\n        if counter &gt; patience:\n            print(f'======== Early stopping at Epoch {epoch} ========')\n            if epoch % print_every != 0:\n                    print(f\"Epoch [{epoch}/{nb_epochs}], Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, Test loss: {test_loss:.3f},\"\n                          f\" Train acc: {train_acc:.2f}, Val acc: {val_acc:.2f}, Test acc: {test_acc:.2f}\")\n            break\n\n    return train_losses, val_losses, test_losses, train_accs, val_accs, test_accs, best_test_acc\n\n\nmodel, optimizer, scheduler, criterion = create_model(Net)\nmodel_5, optimizer_5, scheduler_5, criterion_5 = create_model(Net_complex)\n\nc:\\Users\\wande\\anaconda3\\envs\\gnn\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n\n\n\ntrain_losses, val_losses, test_losses, train_accs, val_accs, test_accs, _ = train(\n    model, criterion, optimizer, scheduler, train_loader, val_loader, test_loader, \n    nb_epochs, patience, device, print_every=1)\n\nEpoch [0/200], Train loss: 4.080, Val loss: 3.995, Test loss: 4.183, Train acc: 0.68, Val acc: 0.56, Test acc: 0.65\nEpoch [1/200], Train loss: 4.002, Val loss: 3.904, Test loss: 4.110, Train acc: 0.61, Val acc: 0.56, Test acc: 0.65\nEpoch [2/200], Train loss: 3.873, Val loss: 3.818, Test loss: 3.998, Train acc: 0.61, Val acc: 0.56, Test acc: 0.65\n\n\nKeyboardInterrupt: \n\n\n\ntrain_losses_5, val_losses_5, test_losses_5, train_accs_5, val_accs_5, test_accs_5, _ = train(\n    model_5, criterion_5, optimizer_5, scheduler_5, train_loader, val_loader, test_loader, \n    nb_epochs, patience, device, print_every=1)\n\nEpoch [0/200], Train loss: 3.583, Val loss: 4.195, Test loss: 4.112, Train acc: 0.60, Val acc: 0.78, Test acc: 0.60\nEpoch [1/200], Train loss: 3.523, Val loss: 4.115, Test loss: 4.071, Train acc: 0.62, Val acc: 0.78, Test acc: 0.60\nEpoch [2/200], Train loss: 3.425, Val loss: 4.060, Test loss: 4.009, Train acc: 0.62, Val acc: 0.78, Test acc: 0.60\nEpoch [3/200], Train loss: 3.358, Val loss: 3.915, Test loss: 3.947, Train acc: 0.63, Val acc: 0.78, Test acc: 0.60\nEpoch [4/200], Train loss: 3.308, Val loss: 3.718, Test loss: 3.786, Train acc: 0.65, Val acc: 0.78, Test acc: 0.60\nEpoch [5/200], Train loss: 3.232, Val loss: 3.583, Test loss: 3.620, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [6/200], Train loss: 3.148, Val loss: 3.412, Test loss: 3.464, Train acc: 0.62, Val acc: 0.78, Test acc: 0.60\nEpoch [7/200], Train loss: 3.075, Val loss: 3.277, Test loss: 3.345, Train acc: 0.65, Val acc: 0.78, Test acc: 0.60\nEpoch [8/200], Train loss: 2.949, Val loss: 3.152, Test loss: 3.214, Train acc: 0.65, Val acc: 0.78, Test acc: 0.60\nEpoch [9/200], Train loss: 2.830, Val loss: 3.002, Test loss: 3.054, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [10/200], Train loss: 2.732, Val loss: 2.825, Test loss: 2.880, Train acc: 0.67, Val acc: 0.78, Test acc: 0.60\nEpoch [11/200], Train loss: 2.582, Val loss: 2.645, Test loss: 2.710, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [12/200], Train loss: 2.415, Val loss: 2.447, Test loss: 2.517, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [13/200], Train loss: 2.228, Val loss: 2.231, Test loss: 2.296, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [14/200], Train loss: 2.042, Val loss: 2.011, Test loss: 2.080, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [15/200], Train loss: 1.816, Val loss: 1.775, Test loss: 1.845, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [16/200], Train loss: 1.651, Val loss: 1.523, Test loss: 1.595, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [17/200], Train loss: 1.448, Val loss: 1.327, Test loss: 1.415, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [18/200], Train loss: 1.287, Val loss: 1.190, Test loss: 1.255, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [19/200], Train loss: 1.183, Val loss: 1.091, Test loss: 1.149, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [20/200], Train loss: 1.053, Val loss: 0.986, Test loss: 1.043, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [21/200], Train loss: 0.978, Val loss: 0.911, Test loss: 0.961, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [22/200], Train loss: 0.936, Val loss: 0.894, Test loss: 0.932, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [23/200], Train loss: 0.896, Val loss: 0.833, Test loss: 0.879, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [24/200], Train loss: 0.885, Val loss: 0.786, Test loss: 0.835, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [25/200], Train loss: 0.830, Val loss: 0.787, Test loss: 0.826, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [26/200], Train loss: 0.840, Val loss: 0.746, Test loss: 0.797, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [27/200], Train loss: 0.816, Val loss: 0.714, Test loss: 0.779, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [28/200], Train loss: 0.809, Val loss: 0.708, Test loss: 0.762, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [29/200], Train loss: 0.775, Val loss: 0.678, Test loss: 0.751, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [30/200], Train loss: 0.765, Val loss: 0.643, Test loss: 0.715, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [31/200], Train loss: 0.788, Val loss: 0.642, Test loss: 0.715, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [32/200], Train loss: 0.776, Val loss: 0.639, Test loss: 0.704, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [33/200], Train loss: 0.764, Val loss: 0.624, Test loss: 0.702, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [34/200], Train loss: 0.782, Val loss: 0.627, Test loss: 0.695, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [35/200], Train loss: 0.769, Val loss: 0.638, Test loss: 0.697, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [36/200], Train loss: 0.747, Val loss: 0.613, Test loss: 0.692, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [37/200], Train loss: 0.741, Val loss: 0.620, Test loss: 0.693, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [38/200], Train loss: 0.751, Val loss: 0.617, Test loss: 0.696, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [39/200], Train loss: 0.740, Val loss: 0.628, Test loss: 0.682, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [40/200], Train loss: 0.746, Val loss: 0.634, Test loss: 0.684, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [41/200], Train loss: 0.762, Val loss: 0.627, Test loss: 0.679, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [42/200], Train loss: 0.733, Val loss: 0.621, Test loss: 0.678, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [43/200], Train loss: 0.719, Val loss: 0.653, Test loss: 0.702, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [44/200], Train loss: 0.737, Val loss: 0.617, Test loss: 0.707, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [45/200], Train loss: 0.714, Val loss: 0.644, Test loss: 0.705, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [46/200], Train loss: 0.706, Val loss: 0.650, Test loss: 0.687, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [47/200], Train loss: 0.701, Val loss: 0.677, Test loss: 0.706, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [48/200], Train loss: 0.725, Val loss: 0.660, Test loss: 0.703, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [49/200], Train loss: 0.688, Val loss: 0.636, Test loss: 0.698, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [50/200], Train loss: 0.714, Val loss: 0.652, Test loss: 0.699, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [51/200], Train loss: 0.679, Val loss: 0.646, Test loss: 0.687, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [52/200], Train loss: 0.711, Val loss: 0.660, Test loss: 0.694, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [53/200], Train loss: 0.698, Val loss: 0.675, Test loss: 0.713, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [54/200], Train loss: 0.699, Val loss: 0.674, Test loss: 0.716, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [55/200], Train loss: 0.679, Val loss: 0.667, Test loss: 0.705, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [56/200], Train loss: 0.693, Val loss: 0.655, Test loss: 0.718, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [57/200], Train loss: 0.689, Val loss: 0.653, Test loss: 0.708, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [58/200], Train loss: 0.672, Val loss: 0.664, Test loss: 0.723, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [59/200], Train loss: 0.698, Val loss: 0.653, Test loss: 0.721, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [60/200], Train loss: 0.687, Val loss: 0.653, Test loss: 0.715, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [61/200], Train loss: 0.686, Val loss: 0.655, Test loss: 0.706, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [62/200], Train loss: 0.653, Val loss: 0.639, Test loss: 0.712, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [63/200], Train loss: 0.673, Val loss: 0.632, Test loss: 0.712, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [64/200], Train loss: 0.664, Val loss: 0.635, Test loss: 0.723, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [65/200], Train loss: 0.659, Val loss: 0.613, Test loss: 0.707, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [66/200], Train loss: 0.667, Val loss: 0.609, Test loss: 0.715, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [67/200], Train loss: 0.695, Val loss: 0.649, Test loss: 0.726, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [68/200], Train loss: 0.660, Val loss: 0.661, Test loss: 0.728, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [69/200], Train loss: 0.670, Val loss: 0.669, Test loss: 0.754, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [70/200], Train loss: 0.660, Val loss: 0.690, Test loss: 0.728, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [71/200], Train loss: 0.647, Val loss: 0.680, Test loss: 0.743, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [72/200], Train loss: 0.667, Val loss: 0.673, Test loss: 0.728, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [73/200], Train loss: 0.644, Val loss: 0.681, Test loss: 0.738, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [74/200], Train loss: 0.667, Val loss: 0.671, Test loss: 0.748, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [75/200], Train loss: 0.673, Val loss: 0.681, Test loss: 0.751, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [76/200], Train loss: 0.662, Val loss: 0.673, Test loss: 0.726, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [77/200], Train loss: 0.627, Val loss: 0.678, Test loss: 0.723, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [78/200], Train loss: 0.617, Val loss: 0.700, Test loss: 0.719, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [79/200], Train loss: 0.658, Val loss: 0.690, Test loss: 0.722, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [80/200], Train loss: 0.644, Val loss: 0.670, Test loss: 0.714, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [81/200], Train loss: 0.628, Val loss: 0.679, Test loss: 0.712, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [82/200], Train loss: 0.635, Val loss: 0.678, Test loss: 0.721, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [83/200], Train loss: 0.612, Val loss: 0.689, Test loss: 0.715, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [84/200], Train loss: 0.627, Val loss: 0.656, Test loss: 0.697, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [85/200], Train loss: 0.617, Val loss: 0.686, Test loss: 0.731, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [86/200], Train loss: 0.655, Val loss: 0.689, Test loss: 0.725, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [87/200], Train loss: 0.652, Val loss: 0.678, Test loss: 0.735, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [88/200], Train loss: 0.660, Val loss: 0.711, Test loss: 0.726, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [89/200], Train loss: 0.657, Val loss: 0.703, Test loss: 0.736, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [90/200], Train loss: 0.629, Val loss: 0.692, Test loss: 0.732, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [91/200], Train loss: 0.609, Val loss: 0.673, Test loss: 0.757, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [92/200], Train loss: 0.652, Val loss: 0.689, Test loss: 0.724, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [93/200], Train loss: 0.616, Val loss: 0.728, Test loss: 0.766, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [94/200], Train loss: 0.640, Val loss: 0.686, Test loss: 0.750, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [95/200], Train loss: 0.611, Val loss: 0.680, Test loss: 0.717, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [96/200], Train loss: 0.626, Val loss: 0.695, Test loss: 0.718, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [97/200], Train loss: 0.606, Val loss: 0.708, Test loss: 0.722, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [98/200], Train loss: 0.592, Val loss: 0.689, Test loss: 0.732, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [99/200], Train loss: 0.622, Val loss: 0.674, Test loss: 0.718, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [100/200], Train loss: 0.631, Val loss: 0.719, Test loss: 0.732, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [101/200], Train loss: 0.607, Val loss: 0.682, Test loss: 0.727, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [102/200], Train loss: 0.617, Val loss: 0.656, Test loss: 0.697, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [103/200], Train loss: 0.582, Val loss: 0.657, Test loss: 0.730, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [104/200], Train loss: 0.596, Val loss: 0.663, Test loss: 0.733, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [105/200], Train loss: 0.616, Val loss: 0.686, Test loss: 0.730, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [106/200], Train loss: 0.602, Val loss: 0.681, Test loss: 0.694, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [107/200], Train loss: 0.593, Val loss: 0.660, Test loss: 0.704, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [108/200], Train loss: 0.622, Val loss: 0.690, Test loss: 0.685, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [109/200], Train loss: 0.591, Val loss: 0.669, Test loss: 0.727, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [110/200], Train loss: 0.569, Val loss: 0.694, Test loss: 0.723, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [111/200], Train loss: 0.578, Val loss: 0.711, Test loss: 0.781, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [112/200], Train loss: 0.577, Val loss: 0.697, Test loss: 0.732, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [113/200], Train loss: 0.561, Val loss: 0.695, Test loss: 0.706, Train acc: 0.67, Val acc: 0.78, Test acc: 0.60\nEpoch [114/200], Train loss: 0.596, Val loss: 0.648, Test loss: 0.703, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [115/200], Train loss: 0.570, Val loss: 0.630, Test loss: 0.688, Train acc: 0.66, Val acc: 0.78, Test acc: 0.60\nEpoch [116/200], Train loss: 0.573, Val loss: 0.659, Test loss: 0.686, Train acc: 0.65, Val acc: 0.78, Test acc: 0.60\nEpoch [117/200], Train loss: 0.568, Val loss: 0.663, Test loss: 0.697, Train acc: 0.67, Val acc: 0.78, Test acc: 0.60\n======== Early stopping at Epoch 117 ========\n\n\n\n\nPlot metrics\n\ndef plot_metrics(train_losses, val_losses, test_losses, train_accs, val_accs, test_accs, suptitle=''): \n    fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n    axs[0].plot(train_losses, label='Train Loss')\n    axs[0].plot(val_losses, label='Val Loss')\n    axs[0].plot(test_losses, label='Test Loss')\n    axs[0].set_title('Loss')\n    axs[0].set_xlabel('Epoch')\n    axs[0].legend()\n    axs[0].grid('on')\n    \n    # Plot accuracies\n    axs[1].plot(train_accs, label='Train Acc')\n    axs[1].plot(val_accs, label='Val Acc')\n    axs[1].plot(test_accs, label='Test Acc')\n    axs[1].set_title('Accuracy')\n    axs[1].set_xlabel('Epoch')\n    axs[1].legend()\n    axs[1].grid('on')\n    \n    plt.tight_layout()\n    plt.suptitle(suptitle)\n    \n    plt.show()\n\n\nplot_metrics(train_losses, val_losses, test_losses, train_accs, val_accs, test_accs, suptitle='Model 1')\n\n\n\n\n\n\n\n\n\nplot_metrics(train_losses_5, val_losses_5, test_losses_5, train_accs_5, val_accs_5, test_accs_5, suptitle='Model - 5 GinConv layers')\n\n\n\n\n\n\n\n\n\n\n10-fold validation\n\ndef k_fold_loader(dataset, fold, k=10, batch_size=BATCH_SIZE):\n    n = len(dataset)//10\n    start = n*fold\n    end = len(dataset) if fold == k else (fold+1)*n \n    indices = list(range(len(dataset)))\n    indices_test = indices[start:end]\n    indices_val_train = indices[:start] + indices[end:]\n    indices_val = np.random.choice(indices_val_train, size=len(indices_val_train)//10)\n    indices_train = [x for x in indices_val_train if x not in indices_val]\n\n    train_dataset = dataset[indices_train]\n    val_dataset = dataset[indices_val]\n    test_dataset = dataset[indices_test]\n\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\n    return train_loader, val_loader, test_loader\n\n\nBATCH_SIZE = 20\nVAL_PCT = 0.15\nTEST_PCT = 0.1\n\nhidden_dim = 32\nnb_epochs = 200\nlearning_rate = 1e-3\nweight_decay = 1e-5\nscheduler_patience = 10\nscheduler_factor = 1e-1\npatience = 50\n\n\ndef run_all(model_architecture, fold, batch_size=BATCH_SIZE, val_pct=VAL_PCT, test_pct=TEST_PCT, input_dim=input_dim, hidden_dim=hidden_dim, num_classes=num_classes, device=device, \n                                learning_rate=learning_rate, weight_decay=weight_decay, scheduler_patience=scheduler_patience, scheduler_factor=scheduler_factor,\n                                nb_epochs=nb_epochs, patience=patience):\n\n    train_loader, val_loader, test_loader = k_fold_loader(dataset, fold, k=10, batch_size=BATCH_SIZE)\n    model, optimizer, scheduler, criterion = create_model(model_architecture, input_dim=input_dim, hidden_dim=hidden_dim, num_classes=num_classes, device=device, \n                                learning_rate=learning_rate, weight_decay=weight_decay, scheduler_patience=scheduler_patience, scheduler_factor=scheduler_factor, verbose_scheduler=False)\n    train_losses, val_losses, test_losses, train_accs, val_accs, test_accs, best_test_acc = train(model, criterion, optimizer, scheduler, train_loader, val_loader, test_loader, \n                        nb_epochs, patience, device, print_every=nb_epochs)\n    return train_losses, val_losses, test_losses, train_accs, val_accs, test_accs, best_test_acc\n\n\ndef run_10_fold(model_architecture, batch_size=BATCH_SIZE, val_pct=VAL_PCT, test_pct=TEST_PCT, input_dim=input_dim, hidden_dim=hidden_dim, num_classes=num_classes, device=device, \n                                learning_rate=learning_rate, weight_decay=weight_decay, scheduler_patience=scheduler_patience, scheduler_factor=scheduler_factor,\n                                nb_epochs=nb_epochs, patience=patience, plot_=None):\n\n    results = {}\n    index_best_test_acc_all_folds = -1\n    best_test_acc_all_folds = 0\n    for i in range(10):\n        print(f'Running fold {i}')\n        train_losses, val_losses, test_losses, train_accs, val_accs, test_accs, best_test_acc = run_all(model_architecture, i, batch_size=batch_size, val_pct=val_pct, test_pct=test_pct, input_dim=input_dim, hidden_dim=hidden_dim, num_classes=num_classes, device=device, \n                                learning_rate=learning_rate, weight_decay=weight_decay, scheduler_patience=scheduler_patience, scheduler_factor=scheduler_factor,\n                                nb_epochs=nb_epochs, patience=patience)\n        results[i] = train_losses, val_losses, test_losses, train_accs, val_accs, test_accs, best_test_acc\n        if best_test_acc_all_folds &lt; best_test_acc:\n            best_test_acc_all_folds = best_test_acc\n            index_best_test_acc_all_folds = i\n\n    if plot_ is not None:\n        train_losses, val_losses, test_losses, train_accs, val_accs, test_accs, _ = results[index_best_test_acc_all_folds]\n        plot_metrics(train_losses, val_losses, test_losses, train_accs, val_accs, test_accs, suptitle=plot_)\n    return results\n\n\nresults = run_10_fold(Net, plot_='Model 1')\n\nRunning fold 0\nEpoch [0/200], Train loss: 3.829, Val loss: 3.345, Test loss: 3.199, Train acc: 0.66, Val acc: 0.71, Test acc: 0.67\n======== Early stopping at Epoch 110 ========\nEpoch [110/200], Train loss: 0.482, Val loss: 0.567, Test loss: 0.375, Train acc: 0.80, Val acc: 0.76, Test acc: 0.89\nRunning fold 1\nEpoch [0/200], Train loss: 4.054, Val loss: 3.511, Test loss: 3.974, Train acc: 0.43, Val acc: 0.65, Test acc: 0.33\n======== Early stopping at Epoch 116 ========\nEpoch [116/200], Train loss: 0.364, Val loss: 0.447, Test loss: 0.550, Train acc: 0.84, Val acc: 0.82, Test acc: 0.78\nRunning fold 2\nEpoch [0/200], Train loss: 3.886, Val loss: 3.619, Test loss: 3.519, Train acc: 0.56, Val acc: 0.76, Test acc: 0.56\n======== Early stopping at Epoch 66 ========\nEpoch [66/200], Train loss: 0.433, Val loss: 0.513, Test loss: 0.447, Train acc: 0.80, Val acc: 0.71, Test acc: 0.78\nRunning fold 3\nEpoch [0/200], Train loss: 3.244, Val loss: 2.872, Test loss: 2.937, Train acc: 0.56, Val acc: 0.71, Test acc: 0.61\n======== Early stopping at Epoch 97 ========\nEpoch [97/200], Train loss: 0.452, Val loss: 0.342, Test loss: 0.252, Train acc: 0.77, Val acc: 0.94, Test acc: 1.00\nRunning fold 4\nEpoch [0/200], Train loss: 4.239, Val loss: 3.605, Test loss: 4.701, Train acc: 0.65, Val acc: 0.53, Test acc: 0.78\n======== Early stopping at Epoch 111 ========\nEpoch [111/200], Train loss: 0.429, Val loss: 0.364, Test loss: 0.693, Train acc: 0.80, Val acc: 0.94, Test acc: 0.67\nRunning fold 5\nEpoch [0/200], Train loss: 3.624, Val loss: 3.493, Test loss: 3.546, Train acc: 0.37, Val acc: 0.24, Test acc: 0.22\n======== Early stopping at Epoch 84 ========\nEpoch [84/200], Train loss: 0.450, Val loss: 0.604, Test loss: 0.370, Train acc: 0.80, Val acc: 0.82, Test acc: 0.89\nRunning fold 6\nEpoch [0/200], Train loss: 4.061, Val loss: 3.697, Test loss: 3.498, Train acc: 0.40, Val acc: 0.24, Test acc: 0.39\n======== Early stopping at Epoch 100 ========\nEpoch [100/200], Train loss: 0.437, Val loss: 0.461, Test loss: 0.718, Train acc: 0.85, Val acc: 0.71, Test acc: 0.67\nRunning fold 7\nEpoch [0/200], Train loss: 3.738, Val loss: 3.287, Test loss: 3.286, Train acc: 0.65, Val acc: 0.82, Test acc: 0.67\n======== Early stopping at Epoch 172 ========\nEpoch [172/200], Train loss: 0.408, Val loss: 0.387, Test loss: 0.457, Train acc: 0.82, Val acc: 0.88, Test acc: 0.83\nRunning fold 8\nEpoch [0/200], Train loss: 4.053, Val loss: 4.000, Test loss: 4.020, Train acc: 0.64, Val acc: 0.65, Test acc: 0.78\n======== Early stopping at Epoch 98 ========\nEpoch [98/200], Train loss: 0.431, Val loss: 0.941, Test loss: 0.509, Train acc: 0.79, Val acc: 0.71, Test acc: 0.83\nRunning fold 9\nEpoch [0/200], Train loss: 3.821, Val loss: 3.376, Test loss: 3.434, Train acc: 0.44, Val acc: 0.41, Test acc: 0.39\n======== Early stopping at Epoch 117 ========\nEpoch [117/200], Train loss: 0.408, Val loss: 0.378, Test loss: 0.545, Train acc: 0.83, Val acc: 0.88, Test acc: 0.83\n\n\n\n\n\n\n\n\n\n\nresults_5 = run_10_fold(Net_complex, plot_='Model - 5 GinConv layers')\n\nRunning fold 0\nEpoch [0/200], Train loss: 3.862, Val loss: 3.237, Test loss: 3.277, Train acc: 0.64, Val acc: 0.88, Test acc: 0.67\n======== Early stopping at Epoch 72 ========\nEpoch [72/200], Train loss: 0.443, Val loss: 0.721, Test loss: 0.534, Train acc: 0.80, Val acc: 0.47, Test acc: 0.67\nRunning fold 1\nEpoch [0/200], Train loss: 3.777, Val loss: 3.742, Test loss: 3.884, Train acc: 0.66, Val acc: 0.71, Test acc: 0.67\n======== Early stopping at Epoch 69 ========\nEpoch [69/200], Train loss: 0.412, Val loss: 0.646, Test loss: 0.645, Train acc: 0.84, Val acc: 0.59, Test acc: 0.67\nRunning fold 2\nEpoch [0/200], Train loss: 4.169, Val loss: 3.962, Test loss: 3.512, Train acc: 0.66, Val acc: 0.76, Test acc: 0.56\n======== Early stopping at Epoch 162 ========\nEpoch [162/200], Train loss: 0.431, Val loss: 0.591, Test loss: 0.591, Train acc: 0.79, Val acc: 0.76, Test acc: 0.72\nRunning fold 3\nEpoch [0/200], Train loss: 3.997, Val loss: 3.486, Test loss: 3.679, Train acc: 0.34, Val acc: 0.29, Test acc: 0.39\n======== Early stopping at Epoch 71 ========\nEpoch [71/200], Train loss: 0.524, Val loss: 0.593, Test loss: 0.420, Train acc: 0.71, Val acc: 0.59, Test acc: 0.83\nRunning fold 4\nEpoch [0/200], Train loss: 3.927, Val loss: 3.782, Test loss: 4.302, Train acc: 0.42, Val acc: 0.18, Test acc: 0.22\n======== Early stopping at Epoch 134 ========\nEpoch [134/200], Train loss: 0.418, Val loss: 0.385, Test loss: 0.578, Train acc: 0.82, Val acc: 0.82, Test acc: 0.78\nRunning fold 5\nEpoch [0/200], Train loss: 3.564, Val loss: 3.137, Test loss: 3.362, Train acc: 0.66, Val acc: 0.59, Test acc: 0.78\n======== Early stopping at Epoch 137 ========\nEpoch [137/200], Train loss: 0.420, Val loss: 0.565, Test loss: 0.413, Train acc: 0.82, Val acc: 0.76, Test acc: 0.78\nRunning fold 6\nEpoch [0/200], Train loss: 3.705, Val loss: 3.420, Test loss: 3.359, Train acc: 0.67, Val acc: 0.53, Test acc: 0.61\n======== Early stopping at Epoch 170 ========\nEpoch [170/200], Train loss: 0.371, Val loss: 0.624, Test loss: 0.693, Train acc: 0.85, Val acc: 0.71, Test acc: 0.72\nRunning fold 7\nEpoch [0/200], Train loss: 4.519, Val loss: 3.666, Test loss: 3.849, Train acc: 0.69, Val acc: 0.47, Test acc: 0.67\n======== Early stopping at Epoch 104 ========\nEpoch [104/200], Train loss: 0.410, Val loss: 0.568, Test loss: 0.469, Train acc: 0.83, Val acc: 0.71, Test acc: 0.83\nRunning fold 8\nEpoch [0/200], Train loss: 3.668, Val loss: 3.080, Test loss: 3.429, Train acc: 0.43, Val acc: 0.35, Test acc: 0.22\n======== Early stopping at Epoch 137 ========\nEpoch [137/200], Train loss: 0.459, Val loss: 0.472, Test loss: 0.545, Train acc: 0.75, Val acc: 0.76, Test acc: 0.83\nRunning fold 9\nEpoch [0/200], Train loss: 4.072, Val loss: 3.747, Test loss: 3.781, Train acc: 0.38, Val acc: 0.12, Test acc: 0.39\n======== Early stopping at Epoch 80 ========\nEpoch [80/200], Train loss: 0.426, Val loss: 0.571, Test loss: 0.400, Train acc: 0.81, Val acc: 0.65, Test acc: 0.89\n\n\n\n\n\n\n\n\n\n\ntest_accuracy_results = np.mean([results[i][5][-1] for i in range(len(results))])\nbest_test_accuracy_results = np.mean([results[i][6] for i in range(len(results))])\n\nprint('Simple model')\nprint(f'Average accuracy at last epoch: {round(test_accuracy_results*100, 2)}%')\nprint(f'Average accuracy at best epoch: {round(best_test_accuracy_results*100, 2)}%')\n\nSimple model\nAverage accuracy at last epoch: 81.67%\nAverage accuracy at best epoch: 82.22%\n\n\n\ntest_accuracy_results_5 = np.mean([results_5[i][5][-1] for i in range(len(results_5))])\nbest_test_accuracy_results_5 = np.mean([results_5[i][6] for i in range(len(results_5))])\n\nprint('Complex model')\nprint(f'Average accuracy at last epoch: {round(test_accuracy_results_5*100, 2)}%')\nprint(f'Average accuracy at best epoch: {round(best_test_accuracy_results_5*100, 2)}%')\n\nComplex model\nAverage accuracy at last epoch: 77.22%\nAverage accuracy at best epoch: 79.44%"
  },
  {
    "objectID": "notebooks/Homophily.html",
    "href": "notebooks/Homophily.html",
    "title": "Manually",
    "section": "",
    "text": "# Install required packages.\nimport os\nimport torch\nimport numpy as np\nimport networkx as nx\nimport torch_geometric\nfrom torch_geometric.datasets import TUDataset\nfrom torch_geometric.datasets import GNNBenchmarkDataset\nfrom torch_geometric.utils import homophily\nimport csv\nimport sys\nos.environ['TORCH'] = torch.__version__\nprint(torch.__version__)\nsys.path.append('../')\nfrom data.homophily.homophily_functions import get_homophily\n\n2.2.1+cpu\ndataset = TUDataset(root='data/TUDataset', name='MUTAG')\n\ntorch.manual_seed(12345)\ndataset = dataset.shuffle()\n\n\ntrain_dataset = dataset[:150]\ntest_dataset = dataset[150:]\n\nprint(f'Number of training graphs: {len(train_dataset)}')\nprint(f'Number of test graphs: {len(test_dataset)}')\n\nDownloading https://www.chrsmrrs.com/graphkerneldatasets/MUTAG.zip\nProcessing...\nDone!\n\n\nNumber of training graphs: 150\nNumber of test graphs: 38\n# To plot the graph\ngraph = train_dataset[0]\ng = torch_geometric.utils.to_networkx(graph, to_undirected=True)\nnx.draw_networkx(g)\n# To see the features\ngraph.x\n\ntensor([[1., 0., 0., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0., 0., 0.],\n        [0., 1., 0., 0., 0., 0., 0.],\n        [0., 0., 1., 0., 0., 0., 0.],\n        [0., 0., 1., 0., 0., 0., 0.]])\n# To transform the label into int\ntorch.argmax(graph.x, dim=1)\n\ntensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2])\n# Display the homophily with different formula\nhomophily(graph.edge_index, torch.argmax(graph.x, dim=1), method='edge')\n\n0.8421052694320679\nhomophily(graph.edge_index, torch.argmax(graph.x, dim=1), method='node')\n\n0.8039215803146362\nhomophily(train_dataset.edge_index, torch.argmax(train_dataset.x, dim=1), method='edge')\n\n0.7203870415687561\nhomophily(train_dataset.edge_index, torch.argmax(train_dataset.x, dim=1), method='node')\n\n0.6603721976280212\nhomophily(train_dataset.edge_index, torch.argmax(train_dataset.x, dim=1), method='edge_insensitive')\n\n0.03396705910563469"
  },
  {
    "objectID": "notebooks/Homophily.html#with-our-script",
    "href": "notebooks/Homophily.html#with-our-script",
    "title": "Manually",
    "section": "With our script",
    "text": "With our script\n\nget_homophily(TUDataset, 'TUDataset' , 'Mutag')\n\nName of the dataset: Mutag\nSize of the dataset: 188\nNumber of features: 7\nNumber of classes: 2\nNumber of training graphs: 151\nNumber of test graphs: 37\nHomophily with the edge formula (train/test): 0.721 | 0.722\nHomophily with the node formula (train/test): 0.661 | 0.662\nHomophily with the edge_insensitive formula (train/test): 0.034 | 0.035\nCSV file created successfully: homophily_data.csv\n\n\n\nget_homophily(TUDataset, 'TUDataset' , 'ENZYMES')\n\nName of the dataset: ENZYMES\nSize of the dataset: 600\nNumber of features: 3\nNumber of classes: 6\nNumber of training graphs: 480\nNumber of test graphs: 120\nHomophily with the edge formula (train/test): 0.667 | 0.66\nHomophily with the node formula (train/test): 0.665 | 0.664\nHomophily with the edge_insensitive formula (train/test): 0.37 | 0.399\nCSV file created successfully: homophily_data.csv\n\n\n\nget_homophily(TUDataset, 'TUDataset' , 'PROTEINS')\n\nName of the dataset: PROTEINS\nSize of the dataset: 1113\nNumber of features: 3\nNumber of classes: 2\nNumber of training graphs: 891\nNumber of test graphs: 222\nHomophily with the edge formula (train/test): 0.657 | 0.654\nHomophily with the node formula (train/test): 0.652 | 0.648\nHomophily with the edge_insensitive formula (train/test): 0.372 | 0.367\nCSV file created successfully: homophily_data.csv\n\n\n\nget_homophily(TUDataset, 'TUDataset' , 'NCI1')\n\nDownloading https://www.chrsmrrs.com/graphkerneldatasets/NCI1.zip\nProcessing...\nDone!\n\n\nName of the dataset: NCI1\nSize of the dataset: 4110\nNumber of features: 37\nNumber of classes: 2\nNumber of training graphs: 3288\nNumber of test graphs: 822\nHomophily with the edge formula (train/test): 0.631 | 0.63\nHomophily with the node formula (train/test): 0.587 | 0.586\nHomophily with the edge_insensitive formula (train/test): 0.002 | 0.002\nCSV file created successfully: homophily_data.csv\n\n\n\nget_homophily(GNNBenchmarkDataset, 'GNNBenchmarkDataset' , 'PATTERN')\n\nName of the dataset: PATTERN\nSize of the dataset: 10000\nNumber of features: 3\nNumber of classes: 2\nNumber of training graphs: 8000\nNumber of test graphs: 2000\nHomophily with the edge formula (train/test): 0.333 | 0.333\nHomophily with the node formula (train/test): 0.333 | 0.333\nHomophily with the edge_insensitive formula (train/test): 0.0 | 0.0\nCSV file created successfully: homophily_data.csv\n\n\n\nget_homophily(GNNBenchmarkDataset, 'GNNBenchmarkDataset' , 'CLUSTER')\n\nName of the dataset: CLUSTER\nSize of the dataset: 10000\nNumber of features: 7\nNumber of classes: 6\nNumber of training graphs: 8000\nNumber of test graphs: 2000\nHomophily with the edge formula (train/test): 0.905 | 0.905\nHomophily with the node formula (train/test): 0.901 | 0.901\nHomophily with the edge_insensitive formula (train/test): 0.009 | 0.0\nCSV file created successfully: homophily_data.csv\n\n\n\n#Features continues\n#get_homophily(GNNBenchmarkDataset, 'GNNBenchmarkDataset' , 'MNIST')\n\n\nget_homophily(GNNBenchmarkDataset, 'GNNBenchmarkDataset' , 'CIFAR10')\n\nName of the dataset: CIFAR10\nSize of the dataset: 45000\nNumber of features: 3\nNumber of classes: 10\nNumber of training graphs: 36000\nNumber of test graphs: 9000\nHomophily with the edge formula (train/test): 0.779 | 0.777\nHomophily with the node formula (train/test): 0.781 | 0.779\nHomophily with the edge_insensitive formula (train/test): 0.646 | 0.635\nCSV file created successfully: homophily_data.csv\n\n\n\n#Features continues\n#get_homophily(GNNBenchmarkDataset, 'GNNBenchmarkDataset' , 'TSP')\n\n\n#No features\n#get_homophily(GNNBenchmarkDataset, 'GNNBenchmarkDataset' , 'CSL')\n\n\ncsv_file = \"homophily_data.csv\"\n# Check if the file exists\nfile_exists = os.path.exists(csv_file)\n# Open CSV file in append mode if it exists, otherwise in write mode\nmode = 'a' if file_exists else 'w'\n\ndataset = GNNBenchmarkDataset(root='data/' + str(\"GNNBenchmarkDataset\"), name=\"MNIST\")\n    \nsize_dataset = len(dataset)\nnb_class = dataset.num_classes\nnb_features = dataset.num_features\n\nline_csv = [\n    {\"Name_Dataset\": \"MNIST\", \"Size_dataset\": size_dataset, \n     \"Nb_class\": nb_class, \"Nb_features\": nb_features,\"Seed\": 12345,\n     \"Homophily_edge_train\": None, \"Homophily_edge_test\": None,\n     \"Homophily_node_train\": None, \"Homophily_node_test\": None,\n     \"Homophily_edge_insensitive_train\": None, \"Homophily_edge_insensitive_test\": None}\n        ]\n\n# Writing to CSV file\nwith open(csv_file, mode, newline='') as file:\n    # Define column names\n    fieldnames = [\"Name_Dataset\", \"Size_dataset\", \"Nb_class\", \"Nb_features\",\"Seed\",\n                      \"Homophily_edge_train\", \"Homophily_edge_test\",\n                      \"Homophily_node_train\", \"Homophily_node_test\",\n                      \"Homophily_edge_insensitive_train\", \"Homophily_edge_insensitive_test\"]\n    writer = csv.DictWriter(file, fieldnames=fieldnames)\n\n    # Write header only if file is created newly\n    if not file_exists or os.path.getsize(csv_file) == 0:\n        writer.writeheader()\n\n\n    # Write data rows\n    for row in line_csv:\n        writer.writerow(row)"
  },
  {
    "objectID": "chapters/Framework.html",
    "href": "chapters/Framework.html",
    "title": "Framework",
    "section": "",
    "text": "In this section, we will explain how our model works. We will cover the generic architecture of a graph neural network (GNN), provide definitions for some key concepts, and explain four different pooling techniques.\n\n\nA GNN is a neural network architecture designed to operate on graph-structured data. The generic architecture of a GNN consists of several layers, each of which performs a specific function.\n\n\n\nGeneric architecture of a GNN\n\n\nThe first layer is the input layer, which takes in the graph data and node features. The second layer is the convolution layer, which applies a convolution operation to capture local dependencies between neighboring nodes in the graph. The third layer is the local pooling layer, which reduces the dimensionality of the node representations by pooling them together within local neighborhoods. The fourth layer is the global pooling (readout) layer, which transforms the entire graph into a single vector representation. The final layer is the MLP classification layer, which produces the desired output based on the graph representation.\n\n\n\nIn this section, we will provide definitions for some key concepts used in our model.\n\n\n\\[\\text{Definition 1}\\]\n\n\n\n\\[\\text{Definition 2}\\]\n\n\n\n\\[\\text{Definition 3}\\]\n\n\n\n\nIn this section, we will explain four different pooling techniques used in our model.\n\n\n\n\n\nTop-K Pooling Operator (TopKPool)\n\n\nThe Top-K Pooling operator retains only the top-K nodes with the highest scores, as determined by a scoring function. The scoring function can be based on various factors, such as node degree or feature importance. By retaining only the top-K nodes, Top-K Pooling reduces the dimensionality of the node representations while preserving the most important information.\n\n\n\n\n\n\nSelf-Attention Graph Pooling (SAGPool)\n\n\nSelf-Attention Graph Pooling (SAGPool) is a pooling technique that uses self-attention mechanisms to assign scores to nodes. The scores are used to select the top-K nodes for pooling, similar to Top-K Pooling. However, unlike Top-K Pooling, SAGPool takes into account the relationships between nodes when assigning scores. This allows SAGPool to capture more complex dependencies between nodes.\n\n\n\n\n\n\nMEWIS Pool (Maximum Entropy Weighted Independent Set Pooling)\n\n\nMEWIS Pool is a pooling technique that maximizes the Shannon Entropy of the selected nodes. The Shannon Entropy is a measure of the uncertainty or randomness of a set of nodes. By selecting nodes that maximize the Shannon Entropy, MEWIS Pool aims to capture a diverse set of nodes that cover different regions of the graph.\n\n\n\n\n\n\nEDGE Pooling\n\n\nEDGE Pooling is a pooling technique that pairs nodes based on scores. The scores can be based on various factors, such as node degree or feature importance. EDGE Pooling then merges the paired nodes into a single node, reducing the dimensionality of the node representations. EDGE Pooling can be seen as a form of hierarchical clustering, where nodes are merged based on their similarity.\nEach of these pooling techniques has its own strengths and weaknesses, and the choice of pooling technique depends on the specific characteristics of the graph data and the desired outcome."
  },
  {
    "objectID": "chapters/Framework.html#generic-architecture-of-a-gnn",
    "href": "chapters/Framework.html#generic-architecture-of-a-gnn",
    "title": "Framework",
    "section": "",
    "text": "A GNN is a neural network architecture designed to operate on graph-structured data. The generic architecture of a GNN consists of several layers, each of which performs a specific function.\n\n\n\nGeneric architecture of a GNN\n\n\nThe first layer is the input layer, which takes in the graph data and node features. The second layer is the convolution layer, which applies a convolution operation to capture local dependencies between neighboring nodes in the graph. The third layer is the local pooling layer, which reduces the dimensionality of the node representations by pooling them together within local neighborhoods. The fourth layer is the global pooling (readout) layer, which transforms the entire graph into a single vector representation. The final layer is the MLP classification layer, which produces the desired output based on the graph representation."
  },
  {
    "objectID": "chapters/Framework.html#key-definitions",
    "href": "chapters/Framework.html#key-definitions",
    "title": "Framework",
    "section": "",
    "text": "In this section, we will provide definitions for some key concepts used in our model.\n\n\n\\[\\text{Definition 1}\\]\n\n\n\n\\[\\text{Definition 2}\\]\n\n\n\n\\[\\text{Definition 3}\\]"
  },
  {
    "objectID": "chapters/Framework.html#pooling-techniques",
    "href": "chapters/Framework.html#pooling-techniques",
    "title": "Framework",
    "section": "",
    "text": "In this section, we will explain four different pooling techniques used in our model.\n\n\n\n\n\nTop-K Pooling Operator (TopKPool)\n\n\nThe Top-K Pooling operator retains only the top-K nodes with the highest scores, as determined by a scoring function. The scoring function can be based on various factors, such as node degree or feature importance. By retaining only the top-K nodes, Top-K Pooling reduces the dimensionality of the node representations while preserving the most important information.\n\n\n\n\n\n\nSelf-Attention Graph Pooling (SAGPool)\n\n\nSelf-Attention Graph Pooling (SAGPool) is a pooling technique that uses self-attention mechanisms to assign scores to nodes. The scores are used to select the top-K nodes for pooling, similar to Top-K Pooling. However, unlike Top-K Pooling, SAGPool takes into account the relationships between nodes when assigning scores. This allows SAGPool to capture more complex dependencies between nodes.\n\n\n\n\n\n\nMEWIS Pool (Maximum Entropy Weighted Independent Set Pooling)\n\n\nMEWIS Pool is a pooling technique that maximizes the Shannon Entropy of the selected nodes. The Shannon Entropy is a measure of the uncertainty or randomness of a set of nodes. By selecting nodes that maximize the Shannon Entropy, MEWIS Pool aims to capture a diverse set of nodes that cover different regions of the graph.\n\n\n\n\n\n\nEDGE Pooling\n\n\nEDGE Pooling is a pooling technique that pairs nodes based on scores. The scores can be based on various factors, such as node degree or feature importance. EDGE Pooling then merges the paired nodes into a single node, reducing the dimensionality of the node representations. EDGE Pooling can be seen as a form of hierarchical clustering, where nodes are merged based on their similarity.\nEach of these pooling techniques has its own strengths and weaknesses, and the choice of pooling technique depends on the specific characteristics of the graph data and the desired outcome."
  },
  {
    "objectID": "chapters/Introduction.html",
    "href": "chapters/Introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "In this section we will introduce the concept of graph neural networks and the notion of homophily. These concepts are important for understanding our machine learning model and the techniques we used to develop it."
  },
  {
    "objectID": "chapters/Introduction.html#graph-neural-networks",
    "href": "chapters/Introduction.html#graph-neural-networks",
    "title": "Introduction",
    "section": "Graph Neural Networks",
    "text": "Graph Neural Networks\nGraph neural networks (GNNs) are a type of neural network that operates directly on graph data. Unlike traditional neural networks, which operate on fixed-size vectors, GNNs can handle variable-sized inputs and outputs. This makes them well-suited for tasks such as node classification, link prediction, and graph classification.\nGNNs work by propagating information across the edges of a graph. Each node in the graph is represented by a feature vector, and the GNN updates these feature vectors by aggregating information from neighboring nodes. This process is repeated for multiple layers, allowing the GNN to capture complex patterns in the graph data.\n\n\n\nSource: Lin et al., 2021\n\n\nGNN can be seen as an extension of CNN to any topology."
  },
  {
    "objectID": "chapters/Introduction.html#homophily",
    "href": "chapters/Introduction.html#homophily",
    "title": "Introduction",
    "section": "Homophily",
    "text": "Homophily\nHomophily is the tendency for nodes in a graph to be connected to other nodes that are similar to them. This concept is important in the context of graph neural networks, as it can affect the performance of the model.\nIn a graph with high homophily, nodes that are connected to each other tend to have similar feature vectors. This makes it easier for the GNN to learn patterns in the data and make accurate predictions. However, in a graph with low homophily, connected nodes may have very different feature vectors, which can make it more difficult for the GNN to learn useful representations.\nUnderstanding the level of homophily in a graph is an important step in developing a graph neural network. By taking homophily into account, we can design a model that is tailored to the specific characteristics of the graph data.\n\n\n\nSource: Zheng et al., 2022\n\n\nIn the next section, we will go over the details of our machine learning model and how we used graph neural networks and homophily to make accurate predictions."
  },
  {
    "objectID": "notebooks/wilcoxon_test.html",
    "href": "notebooks/wilcoxon_test.html",
    "title": "Load result files",
    "section": "",
    "text": "import json\nfrom glob import glob\nimport numpy as np\nimport pandas as pd\nresult_files = glob('../model/results/*.json')\ndicts = {}\n\nfor file_ in result_files:\n    with open(file_, 'r') as f:\n        d = json.load(f)\n    dataset = d['dataset']\n    if dicts.get(dataset) is None:\n        dicts[dataset] = [d]\n    else:\n        dicts[dataset].append(d)\ndicts_difference = {}\nfor dataset in dicts.keys():\n    dicts_difference[dataset] = dict()\n\nfor dataset in dicts.keys():\n    architecture_files = dicts[dataset]\n    for architecture_file in architecture_files:\n        architecture = f\"{architecture_file['convolution_layer']}_{architecture_file['local_pooling_layer']}\"\n        if dicts_difference[dataset].get(architecture) is None:\n            dicts_difference[dataset][architecture] = 0\n            \n        if architecture_file['global_pooling_layer'] == 'max':\n            dicts_difference[dataset][architecture] += architecture_file['mean_accuracy']\n        else:\n            dicts_difference[dataset][architecture] -= architecture_file['mean_accuracy']"
  },
  {
    "objectID": "notebooks/wilcoxon_test.html#wilcoxon-test",
    "href": "notebooks/wilcoxon_test.html#wilcoxon-test",
    "title": "Load result files",
    "section": "Wilcoxon test",
    "text": "Wilcoxon test\n\nfrom scipy.stats import wilcoxon\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\ndf_wilcoxon = pd.DataFrame()\ndf_wilcoxon.index = list(dicts_difference.keys())\ndf_wilcoxon['p-value'] = [0]*len(df_wilcoxon)\ndf_wilcoxon['mean_diff'] = [0]*len(df_wilcoxon)\n\n\nfor dataset in dicts_difference.keys():\n    list_differences = list(dicts_difference[dataset].values())\n    mean_difference = np.mean(list_differences)\n    res = wilcoxon(list_differences)\n    print(f\"{dataset}: mean_difference = {mean_difference:.4f}\")\n    print(f\"Wilcoxon test statistic = {res.statistic}, p-value = {res.pvalue:.4f}\")\n    if res.pvalue &gt; 0.05:\n        print(\"Can't reject null hyp. -&gt; samples come from the same distribution\")\n    else:\n        print(\"Null hyp. rejected -&gt; samples come from different distribution\")\n    print()\n    df_wilcoxon.loc[dataset, 'p-value'] = res.pvalue\n    df_wilcoxon.loc[dataset, 'mean_diff'] = mean_difference\n\nMUTAG: mean_difference = -0.0084\nWilcoxon test statistic = 34.5, p-value = 0.2583\nCan't reject null hyp. -&gt; samples come from the same distribution\n\nPROTEINS: mean_difference = 0.0090\nWilcoxon test statistic = 42.0, p-value = 0.3303\nCan't reject null hyp. -&gt; samples come from the same distribution\n\nENZYMES: mean_difference = -0.0104\nWilcoxon test statistic = 37.0, p-value = 0.2078\nCan't reject null hyp. -&gt; samples come from the same distribution\n\nNCI1: mean_difference = 0.6687\nWilcoxon test statistic = 0.0, p-value = 0.0001\nNull hyp. rejected -&gt; samples come from different distribution\n\n\n\n\nfor dataset in dicts.keys():\n    best_acc = 0 \n    best_architecture = ''\n    for d in dicts[dataset]:\n        acc = d['mean_accuracy']\n        if acc &gt;= best_acc:\n            best_acc = acc\n            best_architecture = f\"{d['convolution_layer']}_{d['local_pooling_layer']}_{d['global_pooling_layer']}\"\n    print(f\"{dataset}: best architecture = {best_architecture} -&gt; {best_acc:.3f}\")\n    df_wilcoxon.loc[dataset, 'best_arch'] = best_architecture\n\nMUTAG: best architecture = GINConv_EDGE_max -&gt; 0.847\nPROTEINS: best architecture = GCN_EDGE_max -&gt; 0.753\nENZYMES: best architecture = GINConv_EDGE_mean -&gt; 0.379\nNCI1: best architecture = GINConv_MEWIS_max -&gt; 0.744\n\n\n\ndf_wilcoxon\n\n\n\n\n\n\n\n\n\np-value\nmean_diff\nbest_arch\n\n\n\n\nMUTAG\n0.258251\n-0.008421\nGINConv_EDGE_max\n\n\nPROTEINS\n0.330261\n0.008999\nGCN_EDGE_max\n\n\nENZYMES\n0.207764\n-0.010444\nGINConv_EDGE_mean\n\n\nNCI1\n0.000061\n0.668674\nGINConv_MEWIS_max"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Graph Neural Networks",
    "section": "",
    "text": "We are excited to share our machine learning project with you! In this project, we have developed a model using graph neural networks. Our goal is to make this model accessible to everyone, regardless of their technical background.\n\n\nWe have included detailed explanations of our model and the techniques we used. To learn more, navigate to the “Framework” page.\n\n\n\nWe believe in transparency and reproducibility. That’s why we have created an interface that allows you to input your own data and see the predictions made by our model. To use the interface, simply navigate the “Empirical results” page and follow the instructions.\nThank you for visiting our site, and we hope you find our project interesting and useful!"
  },
  {
    "objectID": "index.html#understand-our-model",
    "href": "index.html#understand-our-model",
    "title": "Graph Neural Networks",
    "section": "",
    "text": "We have included detailed explanations of our model and the techniques we used. To learn more, navigate to the “Framework” page."
  },
  {
    "objectID": "index.html#reproducing-our-results",
    "href": "index.html#reproducing-our-results",
    "title": "Graph Neural Networks",
    "section": "",
    "text": "We believe in transparency and reproducibility. That’s why we have created an interface that allows you to input your own data and see the predictions made by our model. To use the interface, simply navigate the “Empirical results” page and follow the instructions.\nThank you for visiting our site, and we hope you find our project interesting and useful!"
  }
]